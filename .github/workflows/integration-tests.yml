name: Integration Tests

on:
  push:
    branches: [main, staging, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      environment:
        description: "Test environment"
        required: true
        default: "test"
        type: choice
        options:
          - test
          - staging
          - production-sim

jobs:
  integration-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: novex_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    env:
      DATABASE_URL: postgresql://postgres:postgres@localhost:5432/novex_test
      REDIS_URL: redis://localhost:6379/0
      ENVIRONMENT: test
      PYTHONPATH: ${{ github.workspace }}/backend

    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Create virtual environment
        run: |
          python -m venv venv
          source venv/bin/activate
          echo "VIRTUAL_ENV=$PWD/venv" >> $GITHUB_ENV
          echo "$PWD/venv/bin" >> $GITHUB_PATH

      - name: Install dependencies
        run: |
          source venv/bin/activate
          pip install --upgrade pip
          pip install -r backend/requirements.txt
          pip install -r ci-cd/requirements-test.txt
          pip install psycopg2-binary redis

      - name: Setup test database
        run: |
          source venv/bin/activate
          python ci-cd/scripts/setup-test-db.py
        continue-on-error: false

      - name: Run integration tests
        run: |
          source venv/bin/activate
          cd backend

          echo "Running integration tests..."

          # Run tests with coverage
          python -m pytest tests/integration/ \
            --cov=apps \
            --cov-report=xml \
            --cov-report=html \
            --junitxml=integration-results.xml \
            -v \
            --tb=short

          # Simple coverage check
          python -c "import xml.etree.ElementTree as ET; tree = ET.parse('coverage.xml'); cov = float(tree.getroot().attrib['line-rate']) * 100; print(f'Coverage: {cov:.1f}%'); exit(0) if cov >= 70 else exit(1)"

      - name: Run API tests
        run: |
          source venv/bin/activate
          cd backend

          echo "Running API tests..."

          # Start the API server in background
          python main.py &
          SERVER_PID=$!

          # Wait for server to start
          sleep 10

          # Run API tests
          python -m pytest tests/api/ \
            --junitxml=api-test-results.xml \
            -v

          # Kill the server
          kill $SERVER_PID 2>/dev/null || true

      - name: Test database connection
        run: |
          source venv/bin/activate
          python ci-cd/scripts/test-db-connection.py
        continue-on-error: false

      - name: Check essential endpoints
        run: |
          source venv/bin/activate

          # Start the API server in background
          cd backend
          python main.py &
          SERVER_PID=$!

          # Wait for server to start
          sleep 15

          # Check endpoints
          python ci-cd/tests/check-essential-endpoints.py \
            --base-url "http://localhost:8000" \
            --timeout 30

          # Kill the server
          kill $SERVER_PID 2>/dev/null || true

      - name: Upload test reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: integration-reports-${{ github.run_id }}
          path: |
            backend/integration-results.xml
            backend/api-test-results.xml
            backend/coverage.xml
            backend/htmlcov/
          retention-days: 30

      - name: Generate test summary
        uses: test-summary/action@v2
        if: always()
        with:
          paths: |
            backend/integration-results.xml
            backend/api-test-results.xml

      - name: Send notification
        if: always()
        uses: rtCamp/action-slack-notify@v2
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
          SLACK_TITLE: "Integration Tests - ${{ github.ref_name }}"
          SLACK_MESSAGE: "Integration tests ${{ job.status }} for commit ${{ github.sha }}"
          SLACK_COLOR: ${{ job.status == 'success' && 'good' || 'danger' }}
        continue-on-error: true

  performance-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/staging'

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install performance testing tools
        run: |
          pip install locust k6

      - name: Run performance smoke test
        run: |
          echo "Performance tests would run here"
          echo "This is a placeholder for actual performance tests"

      - name: Upload performance report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-reports-${{ github.run_id }}
          path: |
            ci-cd/tests/performance/
          retention-days: 30
